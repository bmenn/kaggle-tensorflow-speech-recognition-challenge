{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silent/projects/kaggle-tensorflow-speech-recognition-challenge/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "keras = tf.keras\n",
    "\n",
    "# Make sure ONLY use this for the wav->tensor conversion\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n",
    "NUM_FOLDS = 5\n",
    "SAMPLE_INPUT_LENGTH = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(directory):\n",
    "    metadata = {}\n",
    "    rel_start = len(directory)\n",
    "    for path in glob.glob(directory + '/**/*.wav'):\n",
    "        rel_path = path[rel_start + 1:]\n",
    "        label, fname = os.path.split(path)\n",
    "        label = os.path.basename(label)\n",
    "        speaker = fname.split('_')[0]\n",
    "        try:\n",
    "            metadata[label][speaker].append(path)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                metadata[label][speaker] = [path]\n",
    "            except KeyError:\n",
    "                metadata[label] = {speaker: [path]}    \n",
    "        \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = collect_files('../data/train/audio')\n",
    "metadata.update(collect_files('../data/samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [[] for _ in range(NUM_FOLDS)]\n",
    "labels_set = set(LABELS)\n",
    "for label, v in metadata.items():\n",
    "    if label == '_background_noise_':\n",
    "        continue\n",
    "    if label not in labels_set:\n",
    "        label = 'unknown'\n",
    "    for speaker, files in v.items():\n",
    "        speaker_hash = hashlib.sha1(speaker.encode('utf-8')).hexdigest()\n",
    "        split_num = int(speaker_hash, 16) % NUM_FOLDS\n",
    "        splits[split_num].extend([(fname, label) for fname in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lookup = dict(zip(LABELS, range(len(LABELS))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = [\n",
    "    [(wavfile.read(fname), label) for fname, label in s]\n",
    "    for s in splits\n",
    "]\n",
    "\n",
    "# resize arrays in place\n",
    "[e[0][1].resize((1, SAMPLE_INPUT_LENGTH)) for s in data_splits for e in s]\n",
    "data_splits = [\n",
    "    [np.array([e[0][0] for e in s]),\n",
    "     np.vstack([e[0][1].astype(np.float) for e in s]),\n",
    "     np.array([label_lookup[e[1]] for e in s])]\n",
    "    for s in data_splits\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silent/projects/kaggle-tensorflow-speech-recognition-challenge/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Normalize values to [-1, 1] and convert labels to one-hot\n",
    "for split in data_splits:\n",
    "    split[1] = split[1] / np.abs(split[1]).max(axis=1)[:, np.newaxis]\n",
    "    one_hot = np.zeros((len(split[2]), len(LABELS)))\n",
    "    one_hot[np.arange(len(split[2])), split[2]] = 1\n",
    "    split[2] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, SAMPLE_INPUT_LENGTH])\n",
    "s = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, len(LABELS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.tensorflow.org/api_guides/python/contrib.signal#Computing_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stfts = tf.contrib.signal.stft(x, frame_length=256, frame_step=128,\n",
    "                               fft_length=1024)\n",
    "power_spectrograms = tf.real(stfts * tf.conj(stfts))\n",
    "magnitude_spectrograms = tf.abs(stfts)\n",
    "log_offset = 1e-6\n",
    "log_magnitude_spectrograms = tf.log(magnitude_spectrograms + log_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spectrogram_bins = magnitude_spectrograms.shape[-1].value\n",
    "lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 64\n",
    "linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n",
    "    num_mel_bins, num_spectrogram_bins, 16000, lower_edge_hertz,\n",
    "    upper_edge_hertz)\n",
    "mel_spectrograms = tf.tensordot(magnitude_spectrograms,\n",
    "                                linear_to_mel_weight_matrix, 1)\n",
    "mel_spectrograms.set_shape(\n",
    "    magnitude_spectrograms.shape[:-1].concatenate(\n",
    "        linear_to_mel_weight_matrix.shape[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_offset = 1e-6\n",
    "log_mel_spectrograms = tf.log(mel_spectrograms + log_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mfccs = 13\n",
    "mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(\n",
    "    log_mel_spectrograms)[..., :num_mfccs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = [mfccs.shape[-2].value, mfccs.shape[-1].value]\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "mfcc_images = tf.reshape(mfccs, [-1] + image_size + [1], name=\"mfcc_resize\")\n",
    "h_conv1 = tf.nn.relu(conv2d(mfcc_images, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_size = h_pool2.shape[-3].value * h_pool2.shape[-2].value * h_pool2.shape[-1].value\n",
    "W_fc1 = weight_variable([flatten_size, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, flatten_size])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, len(LABELS)])\n",
    "b_fc2 = bias_variable([len(LABELS)])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    np.concatenate([d[0] for d in data_splits[1:]]),\n",
    "    np.vstack([d[1] for d in data_splits[1:]]),\n",
    "    np.concatenate([d[2] for d in data_splits[1:]]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num_samples):\n",
    "    batch_index = 0\n",
    "    train_length = len(data_splits[0][0])\n",
    "    order = np.arange(train_length)\n",
    "    np.random.shuffle(order)\n",
    "    while True:\n",
    "        if batch_index + num_samples >= train_length:\n",
    "            np.random.shuffle(order)\n",
    "            batch_index = 0\n",
    "\n",
    "        ret = order[batch_index:batch_index+num_samples] \n",
    "        batch_index += num_samples\n",
    "        yield (data_splits[0][1][ret], data_splits[0][2][ret],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training_accuracy 0.015625\n",
      "step 1000, training_accuracy 0.703125\n",
      "step 2000, training_accuracy 0.625\n",
      "test_accuracy 0.608976\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "cast_float = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "hist = {\n",
    "    'train': [],\n",
    "    'test': None\n",
    "}\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "steps = int(num_epochs * len(data_splits[0][1]) / batch_size)\n",
    "start = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(steps):\n",
    "        batch = next(next_batch(batch_size))\n",
    "        if i % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            hist['train'].append(train_accuracy)\n",
    "            print('step %d, training_accuracy %g' % (i, train_accuracy))\n",
    "        \n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        \n",
    "    test_accuracy = []\n",
    "    for i in range(0, len(test[1]), 1024):\n",
    "        val = accuracy.eval(feed_dict={\n",
    "            x: test[1][i:i+1024], y_: test[2][i:i+1024], keep_prob: 1.0})\n",
    "        test_accuracy.append(val)\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "    hist['test'] = test_accuracy\n",
    "    print('test_accuracy %g' % test_accuracy)\n",
    "stop = time.time()\n",
    "duration = stop - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.346879720687866, 12982, 53739)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(duration, len(data_splits[0][2]), len(test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
